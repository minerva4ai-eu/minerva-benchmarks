/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
LAUNCH_FOLDER: {/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/results/vllm/sharegpt/Llama-3.1-405B/Nodes_16-GPUs_4-TP_64-PP_1/launch-1}
BENCHMARK_FILE: {/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py}
DATASET: {sharegpt}
DATASET_PATH: {/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/datasets/ShareGPT_V3_unfiltered_cleaned_split.json}
Node: lrdn0006 - IP: 10.1.0.6
Node: lrdn0020 - IP: 10.1.0.20
Node: lrdn0024 - IP: 10.1.0.24
Node: lrdn0033 - IP: 10.1.0.33
Node: lrdn0040 - IP: 
Node: lrdn0046 - IP: 10.1.0.46
Node: lrdn0064 - IP: 10.1.0.64
Node: lrdn0079 - IP: 10.1.0.79
Node: lrdn0095 - IP: 10.1.0.95
Node: lrdn0114 - IP: 10.1.0.114
Node: lrdn0153 - IP: 10.1.0.153
Node: lrdn0842 - IP: 10.3.0.122
Node: lrdn0875 - IP: 10.3.0.155
Node: lrdn0892 - IP: 10.3.0.172
Node: lrdn0910 - IP: 10.3.0.190
Node: lrdn0931 - IP: 10.3.0.211
Starting HEAD at lrdn0006
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting workers Nodes
Starting WORKER 1 at lrdn0020
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 2 at lrdn0024
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 3 at lrdn0033
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 4 at lrdn0040
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
2025-09-17 04:34:31,066	INFO usage_lib.py:472 -- Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2025-09-17 04:34:31,066	INFO scripts.py:865 -- [37mLocal node IP[39m: [1m10.1.0.6[22m
2025-09-17 04:34:38,997	SUCC scripts.py:902 -- [32m--------------------[39m
2025-09-17 04:34:38,997	SUCC scripts.py:903 -- [32mRay runtime started.[39m
2025-09-17 04:34:38,997	SUCC scripts.py:904 -- [32m--------------------[39m
2025-09-17 04:34:38,997	INFO scripts.py:906 -- [36mNext steps[39m
2025-09-17 04:34:38,997	INFO scripts.py:909 -- To add another node to this Ray cluster, run
2025-09-17 04:34:38,997	INFO scripts.py:912 -- [1m  ray start --address='10.1.0.6:6379'[22m
2025-09-17 04:34:38,997	INFO scripts.py:921 -- To connect to this Ray cluster:
2025-09-17 04:34:38,998	INFO scripts.py:923 -- [35mimport[39m[26m ray
2025-09-17 04:34:38,998	INFO scripts.py:924 -- ray[35m.[39m[26minit(_node_ip_address[35m=[39m[26m[33m'10.1.0.6'[39m[26m)
2025-09-17 04:34:38,998	INFO scripts.py:936 -- To submit a Ray job using the Ray Jobs CLI:
2025-09-17 04:34:38,998	INFO scripts.py:937 -- [1m  RAY_ADDRESS='http://127.0.0.1:8265' ray job submit --working-dir . -- python my_script.py[22m
2025-09-17 04:34:38,998	INFO scripts.py:946 -- See https://docs.ray.io/en/latest/cluster/running-applications/job-submission/index.html 
2025-09-17 04:34:38,998	INFO scripts.py:950 -- for more information on submitting Ray jobs to the Ray cluster.
2025-09-17 04:34:38,998	INFO scripts.py:955 -- To terminate the Ray runtime, run
2025-09-17 04:34:38,998	INFO scripts.py:956 -- [1m  ray stop[22m
2025-09-17 04:34:38,998	INFO scripts.py:959 -- To view the status of the cluster, use
2025-09-17 04:34:38,998	INFO scripts.py:960 --   [1mray status[22m[26m
2025-09-17 04:34:38,998	INFO scripts.py:964 -- To monitor and debug Ray, view the dashboard at 
2025-09-17 04:34:38,998	INFO scripts.py:965 --   [1m127.0.0.1:8265[22m[26m
2025-09-17 04:34:38,998	INFO scripts.py:972 -- [4mIf connection to the dashboard fails, check your firewall settings and network configuration.[24m
Starting WORKER 5 at lrdn0046
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 6 at lrdn0064
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 7 at lrdn0079
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 8 at lrdn0095
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 9 at lrdn0114
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 10 at lrdn0153
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 11 at lrdn0842
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 12 at lrdn0875
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 13 at lrdn0892
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 14 at lrdn0910
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Starting WORKER 15 at lrdn0931
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
Configuration written to config.json
Ray Cluster started correctly
2025-09-17 04:35:28,809	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.1.0.20[22m
2025-09-17 04:35:32,086	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:32,087	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:32,087	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:32,087	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:32,087	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:32,087	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:32,087	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:32,087	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:35:28,808	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.1.0.79[22m
2025-09-17 04:35:28,808	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.1.0.95[22m
2025-09-17 04:35:32,087	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:32,087	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:32,087	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:32,087	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:32,087	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:32,087	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:32,087	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:32,087	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:32,087	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:32,087	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:28,808	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.1.0.33[22m
2025-09-17 04:35:32,087	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:32,087	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:32,087	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:32,088	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:32,088	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:32,087	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:32,088	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:35:32,088	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:35:32,087	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:32,087	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:32,088	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:28,808	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.1.0.114[22m
2025-09-17 04:35:32,088	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:32,087	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:32,088	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:32,088	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:32,088	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:35:32,088	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:32,088	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:32,088	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:28,809	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.1.0.46[22m
2025-09-17 04:35:28,809	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.1.0.153[22m
2025-09-17 04:35:32,088	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:32,088	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:32,088	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:32,088	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:32,088	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:28,809	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.1.0.24[22m
2025-09-17 04:35:32,088	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:32,088	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:32,088	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:32,087	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:32,088	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:32,088	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:32,088	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:32,088	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:32,088	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:32,088	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:35:32,088	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:32,088	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:32,088	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:32,088	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:32,088	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:32,088	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:35:32,088	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:32,088	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:32,088	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:35:32,088	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:32,088	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:32,088	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:35:28,809	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.1.0.40[22m
2025-09-17 04:35:32,088	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:32,088	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:32,088	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:32,088	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:32,088	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:32,088	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:32,088	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:32,088	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:35:28,808	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.1.0.64[22m
2025-09-17 04:35:32,088	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:32,088	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:32,088	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:32,088	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:32,088	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:32,089	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:32,089	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:32,089	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:35:28,808	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.3.0.122[22m
2025-09-17 04:35:33,091	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:33,091	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:33,092	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:33,092	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:33,092	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:33,092	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:33,092	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:33,092	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:35:34,066	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.3.0.155[22m
2025-09-17 04:35:35,338	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:35:35,338	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:35:35,338	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:35:35,338	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:35:35,338	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:35:35,338	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:35:35,338	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:35:35,339	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Ray Status
======== Autoscaler status: 2025-09-17 04:36:08.813966 ========
Node status
---------------------------------------------------------------
Active:
 1 node_dccbbb15c3692d9cb7b6341237e777f996e411d80e99b9c231d8d58a
 1 node_e3cff53585423292d360b8db9a0bfbd2d55da4e49d15e382f5f0284d
 1 node_bcf09d0f0b8f8463af326e02263e120fc7fc3994b445baa11ca02402
 1 node_04f3f0443c2e77334701471c3b355c73bed34290dc717671fe747df6
 1 node_d56b65eaf14f5982d31ef6ab497ae1c9089a36ad9c53b580d32b7f3c
 1 node_315a630a62f19d630635eebfb5d5100755890025b11f17facf34ef38
 1 node_0557a6d677cb9e1916609ab6002b7014285a93870a9d1a523e29e218
 1 node_cf2ddf19832fb41ba0801c4d930c2255005525bc79d68eb6dea8d4aa
 1 node_6d8686c9c0f8b94dd11e7713d7b39b8be14ab14734a77bafc80fb5d0
 1 node_d90c9049d7e712c9b603afd23106bb8970718d92ef5cc80bf6adba07
 1 node_6ff8296dbe458e92587b78c03ce7624da16ae519c1d21cb2fdc6dada
 1 node_58b60d8aabe9b22870192faf76ce3c45edb569854e1e19ecb830bb4b
 1 node_fc71217b85ba147dec3ef8d2b25f92bd8fedee9c9521b5ecc47df6ea
Pending:
 (no pending nodes)
Recent failures:
 (no failures)

Resources
---------------------------------------------------------------
Usage:
 0.0/416.0 CPU
 0.0/52.0 GPU
 0B/4.41TiB memory
 0B/1.90TiB object_store_memory

Demands:
 (no resource demands)
VLLM serve loading... Head Node Address: 10.1.0.6
2025-09-17 04:36:17,666	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.3.0.211[22m
2025-09-17 04:36:21,281	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:36:21,281	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:36:21,281	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:36:21,281	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:36:21,281	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:36:21,281	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:36:21,281	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:36:21,282	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:36:17,666	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.3.0.190[22m
2025-09-17 04:36:21,281	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:36:21,281	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:36:21,281	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:36:21,281	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:36:21,282	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:36:21,282	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:36:21,282	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:36:21,282	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:36:17,667	INFO scripts.py:1047 -- [37mLocal node IP[39m: [1m10.3.0.172[22m
2025-09-17 04:36:21,283	SUCC scripts.py:1063 -- [32m--------------------[39m
2025-09-17 04:36:21,283	SUCC scripts.py:1064 -- [32mRay runtime started.[39m
2025-09-17 04:36:21,283	SUCC scripts.py:1065 -- [32m--------------------[39m
2025-09-17 04:36:21,283	INFO scripts.py:1067 -- To terminate the Ray runtime, run
2025-09-17 04:36:21,283	INFO scripts.py:1068 -- [1m  ray stop[22m
2025-09-17 04:36:21,283	INFO scripts.py:1076 -- [36m[1m--block[22m[39m
2025-09-17 04:36:21,283	INFO scripts.py:1077 -- This command will now block forever until terminated by a signal.
2025-09-17 04:36:21,283	INFO scripts.py:1080 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2025-09-17 04:36:16,306	INFO cli.py:39 -- [37mJob submission server address[39m: [1mhttp://127.0.0.1:8265[22m
2025-09-17 04:36:19,380	SUCC cli.py:63 -- [32m-------------------------------------------------------[39m
2025-09-17 04:36:19,380	SUCC cli.py:64 -- [32mJob 'raysubmit_A5R76enm9BF1W89Z' submitted successfully[39m
2025-09-17 04:36:19,380	SUCC cli.py:65 -- [32m-------------------------------------------------------[39m
2025-09-17 04:36:19,380	INFO cli.py:289 -- [36mNext steps[39m
2025-09-17 04:36:19,380	INFO cli.py:290 -- Query the logs of the job:
2025-09-17 04:36:19,380	INFO cli.py:292 -- [1mray job logs raysubmit_A5R76enm9BF1W89Z[22m
2025-09-17 04:36:19,380	INFO cli.py:294 -- Query the status of the job:
2025-09-17 04:36:19,380	INFO cli.py:296 -- [1mray job status raysubmit_A5R76enm9BF1W89Z[22m
2025-09-17 04:36:19,380	INFO cli.py:298 -- Request the job to be stopped:
2025-09-17 04:36:19,381	INFO cli.py:300 -- [1mray job stop raysubmit_A5R76enm9BF1W89Z[22m
2025-09-17 04:36:19,383	INFO cli.py:307 -- Tailing logs until the job exits (disable with --no-wait):
2025-09-17 04:36:18,951	INFO job_manager.py:530 -- Runtime env is setting up.
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/bin/python
serve.sh: LAUNCH_FOLDER: /leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/results/vllm/sharegpt/Llama-3.1-405B/Nodes_16-GPUs_4-TP_64-PP_1/launch-1
serve.sh: ADDITIONAL_ARGS: --max-model-len=32000 --cpu-offload-gb=0.5
Waiting for vLLM server to be ready...
Waiting for vLLM server to be ready...
INFO 09-17 04:39:17 [__init__.py:244] Automatically detected platform cuda.
INFO 09-17 04:39:56 [api_server.py:1287] vLLM API server version 0.9.1
INFO 09-17 04:39:57 [cli_args.py:309] non-default args: {'port': 2950, 'model': '/leonardo_scratch/large/userinternal/lcavall1/llama405_mlperf/model//Llama-3.1-405B', 'max_model_len': 32000, 'enforce_eager': True, 'distributed_executor_backend': 'ray', 'tensor_parallel_size': 64, 'swap_space': 2.0, 'cpu_offload_gb': 0.5, 'enable_chunked_prefill': True}
INFO 09-17 04:40:20 [config.py:823] This model supports multiple tasks: {'reward', 'classify', 'embed', 'generate', 'score'}. Defaulting to 'generate'.
INFO 09-17 04:40:20 [config.py:2195] Chunked prefill is enabled with max_num_batched_tokens=2048.
WARNING 09-17 04:40:20 [cuda.py:91] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
WARNING 09-17 04:40:23 [env_override.py:17] NCCL_CUMEM_ENABLE is set to 0, skipping override. This may increase memory overhead with cudagraph+allreduce: https://github.com/NVIDIA/nccl/issues/1234
INFO 09-17 04:40:28 [__init__.py:244] Automatically detected platform cuda.
INFO 09-17 04:40:33 [core.py:455] Waiting for init message from front-end.
INFO 09-17 04:40:34 [core.py:70] Initializing a V1 LLM engine (v0.9.1) with config: model='/leonardo_scratch/large/userinternal/lcavall1/llama405_mlperf/model//Llama-3.1-405B', speculative_config=None, tokenizer='/leonardo_scratch/large/userinternal/lcavall1/llama405_mlperf/model//Llama-3.1-405B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=64, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/leonardo_scratch/large/userinternal/lcavall1/llama405_mlperf/model//Llama-3.1-405B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":0,"local_cache_dir":null}
2025-09-17 04:40:34,002	INFO worker.py:1514 -- Using address 10.1.0.6:6379 set in the environment variable RAY_ADDRESS
2025-09-17 04:40:34,003	INFO worker.py:1654 -- Connecting to existing Ray cluster at address: 10.1.0.6:6379...
2025-09-17 04:40:34,011	INFO worker.py:1832 -- Connected to Ray cluster. View the dashboard at [1m[32mhttp://127.0.0.1:8265 [39m[22m
INFO 09-17 04:40:34 [ray_utils.py:334] No current placement group found. Creating a new placement group.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node e3cff53585423292d360b8db9a0bfbd2d55da4e49d15e382f5f0284d. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node 88e2ccb6915de3e4e4d916fe3ccd750501f947678d9836ca41eb477b. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node bcf09d0f0b8f8463af326e02263e120fc7fc3994b445baa11ca02402. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node e10b752f2ef2ae84dc02107083cf078870d8a4a7320a5e548ab00413. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node cf2ddf19832fb41ba0801c4d930c2255005525bc79d68eb6dea8d4aa. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node 0557a6d677cb9e1916609ab6002b7014285a93870a9d1a523e29e218. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node d90c9049d7e712c9b603afd23106bb8970718d92ef5cc80bf6adba07. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node 58b60d8aabe9b22870192faf76ce3c45edb569854e1e19ecb830bb4b. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node 315a630a62f19d630635eebfb5d5100755890025b11f17facf34ef38. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node ba2eb00000ad5c06f20349b6a3640c19632b1281e6714d3b6b7183ae. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node 04f3f0443c2e77334701471c3b355c73bed34290dc717671fe747df6. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node fc71217b85ba147dec3ef8d2b25f92bd8fedee9c9521b5ecc47df6ea. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node d56b65eaf14f5982d31ef6ab497ae1c9089a36ad9c53b580d32b7f3c. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node 6d8686c9c0f8b94dd11e7713d7b39b8be14ab14734a77bafc80fb5d0. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node 6ff8296dbe458e92587b78c03ce7624da16ae519c1d21cb2fdc6dada. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
WARNING 09-17 04:40:35 [ray_utils.py:198] tensor_parallel_size=64 is bigger than a reserved number of GPUs (4 GPUs) in a node dccbbb15c3692d9cb7b6341237e777f996e411d80e99b9c231d8d58a. Tensor parallel workers can be spread out to 2+ nodes which can degrade the performance unless you have fast interconnect across nodes, like Infiniband. To resolve this issue, make sure you have more than 64 GPUs available at each node.
INFO 09-17 04:40:35 [ray_distributed_executor.py:177] use_ray_spmd_worker: True
[36m(pid=2501972)[0m INFO 09-17 04:40:43 [__init__.py:244] Automatically detected platform cuda.
[36m(pid=4184816, ip=10.1.0.64)[0m INFO 09-17 04:42:56 [__init__.py:244] Automatically detected platform cuda.[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
INFO 09-17 04:43:14 [ray_distributed_executor.py:353] non_carry_over_env_vars from config: set()
INFO 09-17 04:43:14 [ray_distributed_executor.py:355] Copying the following environment variables to workers: ['VLLM_USE_RAY_SPMD_WORKER', 'VLLM_USE_RAY_COMPILED_DAG', 'VLLM_WORKER_MULTIPROC_METHOD', 'VLLM_USE_V1']
INFO 09-17 04:43:14 [ray_distributed_executor.py:358] If certain env vars should NOT be copied to workers, add them to /leonardo/home/userinternal/rscheda0/.config/vllm/ray_non_carry_over_env_vars.json file
[36m(RayWorkerWrapper pid=2501972)[0m WARNING 09-17 04:43:30 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14561451bf50>
[36m(pid=3807874, ip=10.1.0.79)[0m INFO 09-17 04:42:56 [__init__.py:244] Automatically detected platform cuda.[32m [repeated 59x across cluster][0m
[36m(RayWorkerWrapper pid=4184816, ip=10.1.0.64)[0m INFO 09-17 04:43:32 [utils.py:1126] Found nccl from library libnccl.so.2
[36m(RayWorkerWrapper pid=4184816, ip=10.1.0.64)[0m INFO 09-17 04:43:32 [pynccl.py:70] vLLM is using nccl==2.26.2
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO cudaDriverVersion 12020
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib1,ib2,ib3
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Bootstrap: Using ib0:10.128.7.33<0>
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO NCCL version 2.26.2+cuda12.2
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO NCCL_IB_DISABLE set by environment to 0.
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib1,ib2,ib3
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO NCCL_IB_HCA set to mlx5_0,mlx5_1,mlx5_4,mlx5_5
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.7.33<0>
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Using network IB
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO ncclCommInitRank comm 0xe300b80 rank 34 nranks 64 cudaDev 2 nvmlDev 2 busId 8f000 commId 0x11629e249a5949ad - Init START
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO RAS client listening socket at ::1<28028>
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Bootstrap timings total 0.066870 (create 0.000021, send 0.000070, recv 0.000332, ring 0.066206, delay 0.000001)
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Setting affinity for GPU 2 to ffff
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO comm 0xe300b80 rank 34 nRanks 64 nNodes 16 localRanks 4 localRank 2 MNNVL 0
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Trees [0] 35/50/-1->34->2 [1] 35/50/-1->34->2 [2] 35/-1/-1->34->38 [3] 35/-1/-1->34->38
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO P2P Chunksize set to 131072
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4185190 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 13
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4185185 [2] NCCL INFO [Proxy Service] Device 2 CPU core 2
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4185193 [2] NCCL INFO [Proxy Progress] Device 2 CPU core 14
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 00/0 : 31[3] -> 34[2] [receive] via NET/IB/2
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 02/0 : 31[3] -> 34[2] [receive] via NET/IB/2
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 01/0 : 34[2] -> 39[3] [send] via NET/IB/3
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 03/0 : 34[2] -> 39[3] [send] via NET/IB/3
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 00/0 : 34[2] -> 33[1] via P2P/IPC/read
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 02/0 : 34[2] -> 33[1] via P2P/IPC/read
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 00/0 : 34[2] -> 35[3] via P2P/IPC/read
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 01/0 : 34[2] -> 35[3] via P2P/IPC/read
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 02/0 : 34[2] -> 35[3] via P2P/IPC/read
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 03/0 : 34[2] -> 35[3] via P2P/IPC/read
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 02/0 : 34[2] -> 38[2] [send] via NET/IB/2
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 03/0 : 34[2] -> 38[2] [send] via NET/IB/3
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 00/0 : 34[2] -> 50[2] [send] via NET/IB/2
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 01/0 : 34[2] -> 50[2] [send] via NET/IB/3
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 00/0 : 2[2] -> 34[2] [receive] via NET/IB/2
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 01/0 : 2[2] -> 34[2] [receive] via NET/IB/3
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 00/0 : 34[2] -> 2[2] [send] via NET/IB/2
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 01/0 : 34[2] -> 2[2] [send] via NET/IB/3
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 00/0 : 50[2] -> 34[2] [receive] via NET/IB/2
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 01/0 : 50[2] -> 34[2] [receive] via NET/IB/3
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO Channel 02/0 : 38[2] -> 34[2] [receive] via NET/IB/2
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m lrdn0064:4184818:4184818 [2] NCCL INFO C
[36m(RayWorkerWrapper pid=3129981, ip=10.3.0.155)[0m lrdn0875:3129981:3129981 [2] NC
[36m(RayWorkerWrapper pid=3195370, ip=10.1.0.20)[0m lrdn0020:3195370:3195370 [2] NCCL IN
[36m(RayWorkerWrapper pid=3827952, ip=10.3.0.122)[0m lrdn0842:3827952:3827952 [2]
[36m(RayWorkerWrapper pid=4184817, ip=10.1.0.64)[0m lrdn0064:4184817:4184817 [3] NCCL INFO Connected all trees
[36m(RayWorkerWrapper pid=4184817, ip=10.1.0.64)[0m lrdn0064:4184817:4184817 [3] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
[36m(RayWorkerWrapper pid=4184817, ip=10.1.0.64)[0m lrdn0064:4184817:4184817 [3] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
[36m(RayWorkerWrapper pid=4184817, ip=10.1.0.64)[0m lrdn0064:4184817:4184817 [3] NCCL INFO TUNER/
[36m(RayWorkerWrapper pid=3807876, ip=10.1.0.79)[0m lrdn0079:3807876:3807876 [2] NCC
[36m(RayWorkerWrapper pid=3807874, ip=10.1.0.79)[0m lrdn0079:3807874:3807874 [3] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
[36m(RayWorkerWrapper pid=3807874, ip=10.1.0.79)[0m lrdn0079:3807874
[36m(RayWorkerWrapper pid=2324827, ip=10.1.0.114)[0m lrdn0114:2324827:2324827 [3] NCCL INFO ncclCommInitRank com
[36m(RayWorkerWrapper pid=3117481, ip=10.1.0.95)[0m lrdn0095:3117481:3
[36m(RayWorkerWrapper pid=2324826, ip=10.1.0.114)[0m lrdn0114:2324826:
[36m(RayWorkerWrapper pid=3117483, ip=10.1.0.95)[0m lrdn0095:3117483:3117483 [2] NCCL
[36m(RayWorkerWrapper pid=2140313, ip=10.3.0.211)[0m lrdn0931:2140313:2140313 [
[36m(RayWorkerWrapper pid=2140312, ip=10.3.0.211)[0m lrdn0931:2140312:2140312 [3] NCCL INFO ncclCommInitRank comm 0xdf84ca0 rank 63 nranks 64 cudaDev 3 nvmlDev 3 busId c8000 commId 0x11629e249a5949ad - Init COMPLETE
[36m(RayWorkerWrapper pid=2140312, ip=10.3.0.211)[0m lrdn0931:2140312:2140312 [3] NCCL INFO Init timings - ncclCommInitRank: rank 63 nranks 64 total 0.83 (kernels 0.41, alloc 0.10,
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2501972 [0] NCCL INFO Channel 00/04 :  0  3  6  5  4  7 10  9  8 11 14 13 12 15 18 17 16 19 22 21 20 23 26 25 24 27 30 29 28 31 34 33 32 35 38 37 ...
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2501972 [0] NCCL INFO Channel 01/04 :  0  2  7  5  4  6 11  9  8 10 15 13 12 14 19 17 16 18 23 21 20 22 27 25 24 26 31 29 28 30 35 33 32 34 39 37 ...
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2501972 [0] NCCL INFO Channel 02/04 :  0  3  6  5  4  7 10  9  8 11 14 13 12 15 18 17 16 19 22 21 20 23 26 25 24 27 30 29 28 31 34 33 32 35 38 37 ...
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2501972 [0] NCCL INFO Channel 03/04 :  0  2  7  5  4  6 11  9  8 10 15 13 12 14 19 17 16 18 23 21 20 22 27 25 24 26 31 29 28 30 35 33 32 34 39 37 ...
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2501972 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 1 directMode 0
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2501972 [0] NCCL INFO CC Off,
[36m(RayWorkerWrapper pid=2501983)[0m lrdn0006:2501983:2501983 [2] NCCL INFO TUNER/Plugin: Could not find: 
[36m(RayWorkerWrapper pid=2501982)[0m lrdn0006:2501982:2501982 [3] NCCL INFO Init timings - ncclCommInitRank: rank 3 nranks 64 total 0.95 (kernels 0.48, alloc 0.05, bootstrap 0.14, allgathers 0.05, topo 0.06
[36m(RayWorkerWrapper pid=1810894, ip=10.3.0.172)[0m lrdn0892:1810894
[36m(RayWorkerWrapper pid=416790, ip=10.3.0.190)[0m lrdn0910:416790:416790 [3] NCCL INFO ncclCommInitRank comm 0xcc58ce0 rank 59 nranks 64 cudaDev 3 nvmlDev 3 bus
[36m(RayWorkerWrapper pid=744347, ip=10.1.0.33)[0m lrdn0033:744347:744347 [2] NCCL INFO threadThresholds 8/8/64 | 
[36m(RayWorkerWrapper pid=3195369, ip=10.1.0.20)[0m lrdn0020:3195369:3195369 [3] NCCL INFO TUNE
[36m(RayWorkerWrapper pid=3129978, ip=10.3.0.155)[0m lrdn0875:3129978:3129978 [3] NCCL INFO T
[36m(RayWorkerWrapper pid=744348, ip=10.1.0.33)[0m lrdn0033:744348:744348 [3] NCCL INFO ncclCommInitRank comm 0xdc66780 rank 23 nranks 64 cudaDev 3 nvmlDev 3 
[36m(RayWorkerWrapper pid=4104210, ip=10.1.0.153)[0m lrdn0153:4104210:4104210 [3] NCCL I
[36m(RayWorkerWrapper pid=141766, ip=10.1.0.24)[0m lrdn0024:141766:1
[36m(RayWorkerWrapper pid=4104208, ip=10.1.0.153)[0m lrdn0153:4104208:4104208 [2] NCCL INFO Co
[36m(RayWorkerWrapper pid=112489, ip=10.1.0.46)[0m lrdn0046:112489:112
[36m(RayWorkerWrapper pid=2441588, ip=10.1.0.40)[0m lrdn0040:2441588:24
[36m(RayWorkerWrapper pid=3117481, ip=10.1.0.95)[0m lrdn0095:3117481:3117481 [3] NCCL INFO Init timings - ncclCommInitRank: rank 43 nranks 64 total 0.84 (kernels 0.43, alloc 0.04, bootstrap 0.08, allgathers 0.04, topo 0.07, graphs 0.01, connections 0.15, rest 0.02)
[36m(RayWorkerWrapper pid=3117483, ip=10.1.0.95)[0m  INFO Connected all trees
[36m(RayWorkerWrapper pid=2501983)[0m libnccl-tuner.so. Using internal tuner plugin.
[36m(RayWorkerWrapper pid=2501982)[0m , graphs 0.01, connections 0.15, rest 0.01)
[36m(RayWorkerWrapper pid=3807876, ip=10.1.0.79)[0m L INFO Connected all trees
[36m(RayWorkerWrapper pid=2324827, ip=10.1.0.114)[0m m 0xde1f070 rank 7 nranks 64 cudaDev 3 nvmlDev 3 busId c8000 commId 0x11629e249a5949ad - Init COMPLETE
[36m(RayWorkerWrapper pid=2140312, ip=10.3.0.211)[0m  bootstrap 0.05, allgathers 0.04, topo 0.07, graphs 0.01, connections 0.15, rest 0.01)
[36m(RayWorkerWrapper pid=2501972)[0m  workFifoBytes 1048576
[36m(RayWorkerWrapper pid=1810884, ip=10.3.0.172)[0m CL INFO Connected all trees
[36m(RayWorkerWrapper pid=3129981, ip=10.3.0.155)[0m CL INFO Channel 03/0 : 54[2] -> 50[2] [receive] via NET/IB/3
[36m(RayWorkerWrapper pid=3195370, ip=10.1.0.20)[0m FO Channel 01/0 : 14[2] -> 10[2] [send] via NET/IB/3
[36m(RayWorkerWrapper pid=3129978, ip=10.3.0.155)[0m UNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
[36m(RayWorkerWrapper pid=416790, ip=10.3.0.190)[0m Id c8000 commId 0x11629e249a5949ad - Init COMPLETE
[36m(RayWorkerWrapper pid=744347, ip=10.1.0.33)[0m 512/8/64 | 512 | 512
[36m(RayWorkerWrapper pid=3195369, ip=10.1.0.20)[0m R/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
[36m(RayWorkerWrapper pid=744348, ip=10.1.0.33)[0m busId c8000 commId 0x11629e249a5949ad - Init COMPLETE
[36m(RayWorkerWrapper pid=141764, ip=10.1.0.24)[0m NFO Connected all trees
[36m(RayWorkerWrapper pid=4104210, ip=10.1.0.153)[0m NFO ncclCommInitRank comm 0xd59cfa0 rank 11 nranks 64 cudaDev 3 nvmlDev 3 busId c8000 commId 0x11629e249a5949ad - Init COMPLETE
[36m(RayWorkerWrapper pid=4104208, ip=10.1.0.153)[0m nnected all trees
[36m(RayWorkerWrapper pid=4184817, ip=10.1.0.64)[0m Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m hannel 03/0 : 38[2] -> 34[2] [receive] via NET/IB/3
[36m(RayWorkerWrapper pid=4184816, ip=10.1.0.64)[0m WARNING 09-17 04:43:34 [custom_all_reduce.py:85] Custom allreduce is disabled because this process group spans across nodes.
[36m(RayWorkerWrapper pid=1810884, ip=10.3.0.172)[0m INFO 09-17 04:43:35 [parallel_state.py:1065] rank 54 in world size 64 is assigned as DP rank 0, PP rank 0, TP rank 54, EP rank 54
[36m(RayWorkerWrapper pid=2501972)[0m INFO 09-17 04:43:35 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_df6b8231'), local_subscribe_addr='ipc:///tmp/479aff22-b14b-460c-a4fc-5cc1482ef2e7', remote_subscribe_addr='tcp://10.1.0.6:46973', remote_addr_ipv6=False)
[36m(RayWorkerWrapper pid=3827950, ip=10.3.0.122)[0m WARNING 09-17 04:43:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m WARNING 09-17 04:43:30 [utils.py:2737] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x14ef72603f10>[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2501972)[0m INFO 09-17 04:43:35 [gpu_model_runner.py:1595] Starting to load model /leonardo_scratch/large/userinternal/lcavall1/llama405_mlperf/model//Llama-3.1-405B...
[36m(RayWorkerWrapper pid=2324826, ip=10.1.0.114)[0m INFO 09-17 04:43:35 [gpu_model_runner.py:1600] Loading model from scratch...
Ray Cluster started with PID 2497730
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
[36m(RayWorkerWrapper pid=3807876, ip=10.1.0.79)[0m INFO 09-17 04:43:37 [cuda.py:252] Using Flash Attention backend on V1 engine.
[36m(RayWorkerWrapper pid=3827953, ip=10.3.0.122)[0m INFO 09-17 04:43:32 [utils.py:1126] Found nccl from library libnccl.so.2[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=3827953, ip=10.3.0.122)[0m INFO 09-17 04:43:32 [pynccl.py:70] vLLM is using nccl==2.26.2[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   0% Completed | 0/191 [00:00<?, ?it/s]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   1% Completed | 1/191 [01:48<5:43:45, 108.56s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   1% Completed | 2/191 [03:23<5:15:53, 100.28s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   2% Completed | 3/191 [04:11<4:00:40, 76.81s/it]
[36m(RayWorkerWrapper pid=2501972)[0m  
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   2% Completed | 4/191 [05:07<3:32:46, 68.27s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   3% Completed | 5/191 [05:08<2:16:53, 44.16s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   3% Completed | 6/191 [05:09<1:31:17, 29.61s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   4% Completed | 7/191 [06:09<2:00:44, 39.37s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   4% Completed | 8/191 [07:09<2:20:11, 45.97s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   5% Completed | 9/191 [08:15<2:38:46, 52.34s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   5% Completed | 10/191 [08:16<1:49:55, 36.44s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   6% Completed | 11/191 [09:19<2:13:52, 44.62s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   6% Completed | 12/191 [10:54<2:58:55, 59.97s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   7% Completed | 13/191 [10:55<2:04:25, 41.94s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   7% Completed | 14/191 [10:56<1:27:34, 29.68s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   8% Completed | 15/191 [12:25<2:19:16, 47.48s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   8% Completed | 16/191 [12:26<1:37:37, 33.47s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   9% Completed | 17/191 [13:09<1:45:37, 36.42s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:   9% Completed | 18/191 [13:10<1:14:31, 25.85s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  10% Completed | 19/191 [15:05<2:30:40, 52.56s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  10% Completed | 20/191 [16:30<2:57:14, 62.19s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  11% Completed | 21/191 [16:30<2:03:43, 43.67s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  12% Completed | 22/191 [16:31<1:26:30, 30.71s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  12% Completed | 23/191 [17:43<2:00:49, 43.15s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  13% Completed | 24/191 [18:42<2:13:06, 47.82s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  13% Completed | 25/191 [19:50<2:29:39, 54.09s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  14% Completed | 26/191 [19:51<1:44:36, 38.04s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  14% Completed | 27/191 [21:47<2:47:36, 61.32s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  15% Completed | 28/191 [21:47<1:57:02, 43.08s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  15% Completed | 29/191 [21:48<1:22:01, 30.38s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  16% Completed | 30/191 [22:48<1:45:22, 39.27s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  16% Completed | 31/191 [24:45<2:47:14, 62.72s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  17% Completed | 32/191 [26:00<2:55:37, 66.27s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  17% Completed | 33/191 [27:18<3:03:34, 69.71s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  18% Completed | 34/191 [27:19<2:08:35, 49.15s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  18% Completed | 35/191 [27:19<1:29:58, 34.61s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  19% Completed | 36/191 [28:31<1:57:50, 45.61s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  19% Completed | 37/191 [28:32<1:22:34, 32.17s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  20% Completed | 38/191 [29:18<1:33:07, 36.52s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  20% Completed | 39/191 [29:19<1:05:11, 25.73s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  21% Completed | 40/191 [31:16<2:13:48, 53.17s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  21% Completed | 41/191 [31:17<1:33:55, 37.57s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  22% Completed | 42/191 [31:18<1:05:43, 26.47s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  23% Completed | 43/191 [32:44<1:49:50, 44.53s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  23% Completed | 44/191 [32:45<1:17:10, 31.50s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  24% Completed | 45/191 [33:50<1:40:57, 41.49s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  24% Completed | 46/191 [34:59<2:00:01, 49.67s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  25% Completed | 47/191 [36:22<2:23:20, 59.72s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  25% Completed | 48/191 [36:27<1:43:21, 43.37s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  26% Completed | 49/191 [38:01<2:18:10, 58.38s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  26% Completed | 50/191 [38:02<1:36:37, 41.12s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  27% Completed | 51/191 [39:02<1:49:05, 46.75s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  27% Completed | 52/191 [40:46<2:28:38, 64.16s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  28% Completed | 53/191 [40:47<1:43:28, 44.99s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  28% Completed | 54/191 [41:51<1:55:55, 50.77s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  29% Completed | 55/191 [42:59<2:06:50, 55.96s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  29% Completed | 56/191 [44:03<2:11:38, 58.51s/it]
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  30% Completed | 57/191 [44:05<1:32:15, 41.31s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  30% Completed | 58/191 [44:05<1:04:27, 29.08s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  31% Completed | 59/191 [45:07<1:25:23, 38.82s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  31% Completed | 60/191 [46:11<1:41:36, 46.54s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  32% Completed | 61/191 [46:13<1:11:26, 32.98s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  32% Completed | 62/191 [47:28<1:38:07, 45.64s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  33% Completed | 63/191 [48:40<1:54:30, 53.68s/it]
[36m(RayWorkerWrapper pid=3807875, ip=10.1.0.79)[0m INFO 09-17 05:33:26 [default_loader.py:272] Loading weights took 2987.73 seconds
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO cudaDriverVersion 12020[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ib0,ib1,ib2,ib3[32m [repeated 126x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO Bootstrap: Using ib0:10.128.7.233<0>[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO NCCL version 2.26.2+cuda12.2[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO NET/Plugin: Could not find: libnccl-net.so. Using internal net plugin.[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO NCCL_IB_DISABLE set by environment to 0.[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO NCCL_IB_HCA set to mlx5_0,mlx5_1,mlx5_4,mlx5_5[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [RO]; OOB ib0:10.128.7.233<0>[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. [32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO Using network IB[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO ncclCommInitRank comm 0xd5eeeb0 rank 4 nranks 64 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x11629e249a5949ad - Init START[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO RAS client listening socket at ::1<28028>[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO Bootstrap timings total 0.029151 (create 0.000018, send 0.000062, recv 0.000257, ring 0.021388, delay 0.000000)[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO NCCL_CUMEM_ENABLE set by environment to 0.[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO Setting affinity for GPU 0 to ffff[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO NCCL_NVLS_ENABLE set by environment to 0.[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO comm 0xd5eeeb0 rank 4 nRanks 64 nNodes 16 localRanks 4 localRank 0 MNNVL 0[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO Trees [0] 5/-1/-1->4->7 [1] 5/-1/-1->4->7 [2] 5/-1/-1->4->7 [3] 5/-1/-1->4->7[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO P2P Chunksize set to 131072[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2325204 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2325201 [0] NCCL INFO [Proxy Service] Device 0 CPU core 5[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2325212 [0] NCCL INFO [Proxy Progress] Device 0 CPU core 15[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=3827951, ip=10.3.0.122)[0m lrdn0842:3827951:3827951 [3] NCCL INFO Channel 03/0 : 54[2] -> 47[3] [receive] via NET/IB/3[32m [repeated 167x across cluster][0m
[36m(RayWorkerWrapper pid=3827952, ip=10.3.0.122)[0m  NCCL INFO Channel 01/0 : 46[2] -> 42[2] [send] via NET/IB/3[32m [repeated 167x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO Channel 03/0 : 4[0] -> 7[3] via P2P/IPC/read[32m [repeated 474x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=1810884, ip=10.3.0.172)[0m lrdn0892:1810884:1810884 [2] NC
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO Connected all trees[32m [repeated 56x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512[32m [repeated 61x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2441590, ip=10.1.0.40)[0m lrdn0040:2441590:2441590 [2] NCC
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.[32m [repeated 58x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO ncclCommInitRank comm 0xd5eeeb0 rank 4 nranks 64 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x11629e249a5949ad - Init COMPLETE[32m [repeated 59x across cluster][0m
[36m(RayWorkerWrapper pid=416789, ip=10.3.0.190)[0m lrdn0910:416789:416789 [2] NCCL INFO threadThresholds 8/8/64 | 512
[36m(RayWorkerWrapper pid=3827951, ip=10.3.0.122)[0m lrdn0842:3827951:3827951 [3] NCCL INFO TUNE
[36m(RayWorkerWrapper pid=112492, ip=10.1.0.46)[0m lrdn0046:112492:112492 [2] NCCL I[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m lrdn0114:2324825:2324825 [0] NCCL INFO Init timings - ncclCommInitRank: rank 4 nranks 64 total 0.89 (kernels 0.45, alloc 0.11, bootstrap 0.04, allgathers 0.04, topo 0.06, graphs 0.01, connections 0.15, rest 0.01)[32m [repeated 61x across cluster][0m
[36m(RayWorkerWrapper pid=2441590, ip=10.1.0.40)[0m L INFO Connected all trees
[36m(RayWorkerWrapper pid=416789, ip=10.3.0.190)[0m /8/64 | 512 | 512
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
[36m(RayWorkerWrapper pid=3827951, ip=10.3.0.122)[0m R/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
[36m(RayWorkerWrapper pid=112492, ip=10.1.0.46)[0m NFO Connected all trees
[36m(RayWorkerWrapper pid=3827953, ip=10.3.0.122)[0m WARNING 09-17 04:43:34 [custom_all_reduce.py:85] Custom allreduce is disabled because this process group spans across nodes.[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=744348, ip=10.1.0.33)[0m INFO 09-17 04:43:35 [parallel_state.py:1065] rank 23 in world size 64 is assigned as DP rank 0, PP rank 0, TP rank 23, EP rank 23[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2441591, ip=10.1.0.40)[0m WARNING 09-17 04:43:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=416789, ip=10.3.0.190)[0m INFO 09-17 04:43:35 [gpu_model_runner.py:1595] Starting to load model /leonardo_scratch/large/userinternal/lcavall1/llama405_mlperf/model//Llama-3.1-405B...[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=2140313, ip=10.3.0.211)[0m INFO 09-17 04:43:35 [gpu_model_runner.py:1600] Loading model from scratch...[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m INFO 09-17 04:43:37 [cuda.py:252] Using Flash Attention backend on V1 engine.[32m [repeated 63x across cluster][0m
[36m(RayWorkerWrapper pid=3807875, ip=10.1.0.79)[0m INFO 09-17 05:33:26 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 2990.470816 seconds
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  34% Completed | 64/191 [50:01<2:10:47, 61.79s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  34% Completed | 65/191 [51:18<2:19:21, 66.36s/it]
[36m(RayWorkerWrapper pid=3827951, ip=10.3.0.122)[0m INFO 09-17 05:35:11 [default_loader.py:272] Loading weights took 3092.81 seconds
[36m(RayWorkerWrapper pid=3827951, ip=10.3.0.122)[0m INFO 09-17 05:35:12 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 3095.729545 seconds
[36m(RayWorkerWrapper pid=4184819, ip=10.1.0.64)[0m INFO 09-17 05:35:34 [default_loader.py:272] Loading weights took 3116.54 seconds
[36m(RayWorkerWrapper pid=4184819, ip=10.1.0.64)[0m INFO 09-17 05:35:35 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 3119.227974 seconds
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  35% Completed | 66/191 [52:19<2:15:02, 64.82s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  35% Completed | 67/191 [52:20<1:34:25, 45.69s/it]
[36m(RayWorkerWrapper pid=3827950, ip=10.3.0.122)[0m INFO 09-17 05:36:43 [default_loader.py:272] Loading weights took 3185.50 seconds
[36m(RayWorkerWrapper pid=3827952, ip=10.3.0.122)[0m INFO 09-17 05:36:43 [default_loader.py:272] Loading weights took 3185.49 seconds
[36m(RayWorkerWrapper pid=3827950, ip=10.3.0.122)[0m INFO 09-17 05:36:44 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 3188.232796 seconds
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  36% Completed | 68/191 [53:40<1:54:47, 56.00s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  36% Completed | 69/191 [55:03<2:10:01, 63.95s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  37% Completed | 70/191 [55:54<2:01:17, 60.14s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  37% Completed | 71/191 [56:30<1:46:05, 53.04s/it]
[36m(RayWorkerWrapper pid=4184816, ip=10.1.0.64)[0m INFO 09-17 05:40:43 [default_loader.py:272] Loading weights took 3424.88 seconds[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=3827953, ip=10.3.0.122)[0m INFO 09-17 05:36:44 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 3188.235041 seconds[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=3195370, ip=10.1.0.20)[0m INFO 09-17 05:40:53 [default_loader.py:272] Loading weights took 3434.72 seconds[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=4184818, ip=10.1.0.64)[0m INFO 09-17 05:40:43 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 3427.704565 seconds[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  38% Completed | 72/191 [58:55<2:39:24, 80.37s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  38% Completed | 73/191 [59:57<2:27:13, 74.86s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  39% Completed | 74/191 [59:58<1:42:53, 52.76s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  39% Completed | 75/191 [1:01:13<1:54:52, 59.42s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  40% Completed | 76/191 [1:02:38<2:08:49, 67.21s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  40% Completed | 77/191 [1:03:59<2:15:40, 71.41s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  41% Completed | 78/191 [1:04:00<1:34:31, 50.19s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  41% Completed | 79/191 [1:04:01<1:05:55, 35.31s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  42% Completed | 80/191 [1:05:54<1:48:48, 58.82s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  42% Completed | 81/191 [1:05:55<1:15:45, 41.32s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  43% Completed | 82/191 [1:06:10<1:00:44, 33.44s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  43% Completed | 83/191 [1:06:21<48:07, 26.73s/it]
[36m(RayWorkerWrapper pid=2501972)[0m   
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  44% Completed | 84/191 [1:06:21<33:36, 18.84s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  45% Completed | 85/191 [1:06:22<23:37, 13.37s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  45% Completed | 86/191 [1:07:06<39:15, 22.43s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  46% Completed | 87/191 [1:07:51<50:43, 29.26s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  46% Completed | 88/191 [1:10:28<1:56:00, 67.57s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  47% Completed | 89/191 [1:10:28<1:20:40, 47.45s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  47% Completed | 90/191 [1:10:33<58:19, 34.65s/it]
[36m(RayWorkerWrapper pid=2501972)[0m   
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  48% Completed | 91/191 [1:10:38<42:44, 25.64s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  48% Completed | 92/191 [1:10:38<29:50, 18.08s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  49% Completed | 93/191 [1:10:44<23:25, 14.34s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  49% Completed | 94/191 [1:10:48<18:19, 11.33s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  50% Completed | 95/191 [1:10:58<17:21, 10.85s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  50% Completed | 96/191 [1:10:58<12:14,  7.73s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  51% Completed | 97/191 [1:11:00<09:24,  6.00s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  51% Completed | 98/191 [1:11:05<08:33,  5.52s/it]
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  52% Completed | 99/191 [1:11:08<07:30,  4.90s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  52% Completed | 100/191 [1:11:11<06:41,  4.41s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  53% Completed | 101/191 [1:11:20<08:24,  5.61s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  53% Completed | 102/191 [1:11:23<07:26,  5.01s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  54% Completed | 103/191 [1:11:24<05:20,  3.65s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  54% Completed | 104/191 [1:11:36<08:50,  6.10s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  55% Completed | 105/191 [1:11:47<11:09,  7.79s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  55% Completed | 106/191 [1:11:57<11:48,  8.33s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  56% Completed | 107/191 [1:11:57<08:17,  5.93s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  57% Completed | 108/191 [1:11:58<05:53,  4.26s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  57% Completed | 109/191 [1:12:02<05:52,  4.30s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  58% Completed | 110/191 [1:12:02<04:14,  3.14s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  58% Completed | 111/191 [1:12:07<04:34,  3.43s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  59% Completed | 112/191 [1:12:13<05:39,  4.29s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  59% Completed | 113/191 [1:12:22<07:19,  5.63s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  60% Completed | 114/191 [1:12:24<05:56,  4.63s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  60% Completed | 115/191 [1:12:26<05:00,  3.95s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  61% Completed | 116/191 [1:12:31<05:11,  4.15s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  61% Completed | 117/191 [1:12:36<05:30,  4.47s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  62% Completed | 118/191 [1:12:51<09:06,  7.49s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  62% Completed | 119/191 [1:12:53<07:13,  6.02s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  63% Completed | 120/191 [1:12:54<05:10,  4.37s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  63% Completed | 121/191 [1:12:54<03:47,  3.25s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  64% Completed | 122/191 [1:12:58<03:45,  3.26s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  64% Completed | 123/191 [1:12:58<02:46,  2.44s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  65% Completed | 124/191 [1:12:59<02:08,  1.91s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  65% Completed | 125/191 [1:13:03<02:46,  2.52s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  66% Completed | 126/191 [1:13:06<02:51,  2.63s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  66% Completed | 127/191 [1:13:09<03:05,  2.90s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  67% Completed | 128/191 [1:13:10<02:18,  2.20s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  68% Completed | 129/191 [1:13:12<02:11,  2.12s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  68% Completed | 130/191 [1:13:14<02:17,  2.26s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  69% Completed | 131/191 [1:13:23<04:09,  4.16s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  69% Completed | 132/191 [1:13:23<03:01,  3.08s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  70% Completed | 133/191 [1:13:24<02:14,  2.33s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  70% Completed | 134/191 [1:13:27<02:31,  2.66s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  71% Completed | 135/191 [1:13:32<02:54,  3.12s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  71% Completed | 136/191 [1:13:32<02:11,  2.40s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  72% Completed | 137/191 [1:13:37<02:43,  3.03s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  72% Completed | 138/191 [1:13:37<02:00,  2.28s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  73% Completed | 139/191 [1:13:39<01:53,  2.18s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  73% Completed | 140/191 [1:13:40<01:22,  1.63s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  74% Completed | 141/191 [1:13:40<01:05,  1.31s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  74% Completed | 142/191 [1:13:43<01:30,  1.84s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  75% Completed | 143/191 [1:13:46<01:45,  2.19s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  75% Completed | 144/191 [1:13:50<02:00,  2.56s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  76% Completed | 145/191 [1:13:51<01:40,  2.17s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  76% Completed | 146/191 [1:13:53<01:41,  2.25s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  77% Completed | 147/191 [1:13:54<01:18,  1.77s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  77% Completed | 148/191 [1:13:57<01:33,  2.17s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  78% Completed | 149/191 [1:14:06<03:00,  4.30s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  79% Completed | 150/191 [1:14:07<02:10,  3.19s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  79% Completed | 151/191 [1:14:15<03:01,  4.53s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  80% Completed | 152/191 [1:14:20<03:06,  4.78s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  80% Completed | 153/191 [1:14:21<02:21,  3.73s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  81% Completed | 154/191 [1:14:28<02:55,  4.74s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  81% Completed | 155/191 [1:14:37<03:26,  5.74s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  82% Completed | 156/191 [1:14:47<04:10,  7.15s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  82% Completed | 157/191 [1:14:51<03:26,  6.08s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  83% Completed | 158/191 [1:14:51<02:26,  4.45s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  83% Completed | 159/191 [1:14:55<02:20,  4.38s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  84% Completed | 160/191 [1:14:56<01:40,  3.24s/it]
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  84% Completed | 161/191 [1:14:56<01:11,  2.39s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  85% Completed | 162/191 [1:14:59<01:07,  2.33s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  85% Completed | 163/191 [1:15:06<01:50,  3.95s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  86% Completed | 164/191 [1:15:11<01:52,  4.15s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  86% Completed | 165/191 [1:15:15<01:49,  4.21s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  87% Completed | 166/191 [1:15:20<01:52,  4.50s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  87% Completed | 167/191 [1:15:24<01:40,  4.19s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  88% Completed | 168/191 [1:15:28<01:36,  4.20s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  88% Completed | 169/191 [1:15:33<01:35,  4.36s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  89% Completed | 170/191 [1:15:38<01:35,  4.57s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  90% Completed | 171/191 [1:15:41<01:19,  3.98s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  90% Completed | 172/191 [1:15:41<00:56,  2.95s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  91% Completed | 173/191 [1:15:45<00:57,  3.17s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  91% Completed | 174/191 [1:15:50<01:06,  3.90s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  92% Completed | 175/191 [1:15:51<00:47,  2.94s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  92% Completed | 176/191 [1:16:00<01:12,  4.82s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  93% Completed | 177/191 [1:16:04<01:04,  4.63s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  93% Completed | 178/191 [1:16:11<01:05,  5.06s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  94% Completed | 179/191 [1:16:11<00:44,  3.69s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  94% Completed | 180/191 [1:16:16<00:43,  3.95s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  95% Completed | 181/191 [1:16:20<00:41,  4.15s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  95% Completed | 182/191 [1:16:24<00:36,  4.08s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  96% Completed | 183/191 [1:16:25<00:24,  3.02s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  96% Completed | 184/191 [1:16:25<00:16,  2.30s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  97% Completed | 185/191 [1:16:26<00:10,  1.80s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  97% Completed | 186/191 [1:16:37<00:22,  4.57s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  98% Completed | 187/191 [1:16:37<00:13,  3.33s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  98% Completed | 188/191 [1:16:42<00:10,  3.63s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  99% Completed | 189/191 [1:16:47<00:08,  4.04s/it]
[36m(RayWorkerWrapper pid=744345, ip=10.1.0.33)[0m INFO 09-17 06:00:27 [default_loader.py:272] Loading weights took 4609.67 seconds[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=3195371, ip=10.1.0.20)[0m INFO 09-17 05:40:54 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 3437.827179 seconds[32m [repeated 7x across cluster][0m
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards:  99% Completed | 190/191 [1:16:53<00:04,  4.68s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards: 100% Completed | 191/191 [1:16:57<00:00,  4.57s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
Loading safetensors checkpoint shards: 100% Completed | 191/191 [1:16:57<00:00, 24.18s/it]
[36m(RayWorkerWrapper pid=2501972)[0m 
[36m(RayWorkerWrapper pid=2501972)[0m INFO 09-17 06:00:35 [default_loader.py:272] Loading weights took 4617.69 seconds[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=1810883, ip=10.3.0.172)[0m INFO 09-17 06:00:28 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 4612.365576 seconds[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=744346, ip=10.1.0.33)[0m INFO 09-17 06:01:09 [default_loader.py:272] Loading weights took 4651.13 seconds
[36m(RayWorkerWrapper pid=2501972)[0m INFO 09-17 06:00:36 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 4620.342454 seconds
[36m(RayWorkerWrapper pid=744347, ip=10.1.0.33)[0m INFO 09-17 06:01:09 [default_loader.py:272] Loading weights took 4651.16 seconds
[36m(RayWorkerWrapper pid=744346, ip=10.1.0.33)[0m INFO 09-17 06:01:09 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 4653.818005 seconds
[36m(RayWorkerWrapper pid=416790, ip=10.3.0.190)[0m INFO 09-17 06:03:19 [default_loader.py:272] Loading weights took 4780.93 seconds[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=744348, ip=10.1.0.33)[0m INFO 09-17 06:01:09 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 4653.817223 seconds[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=2441591, ip=10.1.0.40)[0m INFO 09-17 06:03:47 [default_loader.py:272] Loading weights took 4809.25 seconds[32m [repeated 4x across cluster][0m
[36m(RayWorkerWrapper pid=1810894, ip=10.3.0.172)[0m INFO 09-17 06:03:20 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 4784.991870 seconds[32m [repeated 4x across cluster][0m
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m INFO 09-17 06:05:39 [default_loader.py:272] Loading weights took 4921.17 seconds
[36m(RayWorkerWrapper pid=2441591, ip=10.1.0.40)[0m INFO 09-17 06:03:48 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 4812.063073 seconds
[36m(RayWorkerWrapper pid=2441588, ip=10.1.0.40)[0m INFO 09-17 06:05:39 [default_loader.py:272] Loading weights took 4921.20 seconds
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m INFO 09-17 06:05:40 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 4924.072843 seconds
[36m(RayWorkerWrapper pid=2140312, ip=10.3.0.211)[0m INFO 09-17 06:06:47 [default_loader.py:272] Loading weights took 4989.63 seconds[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=2441590, ip=10.1.0.40)[0m INFO 09-17 06:05:40 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 4924.073043 seconds[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=416789, ip=10.3.0.190)[0m INFO 09-17 06:08:39 [default_loader.py:272] Loading weights took 5101.68 seconds[32m [repeated 10x across cluster][0m
[36m(RayWorkerWrapper pid=112490, ip=10.1.0.46)[0m INFO 09-17 06:06:52 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 4996.715774 seconds[32m [repeated 10x across cluster][0m
[36m(RayWorkerWrapper pid=416788, ip=10.3.0.190)[0m INFO 09-17 06:08:48 [default_loader.py:272] Loading weights took 5110.64 seconds
[36m(RayWorkerWrapper pid=416789, ip=10.3.0.190)[0m INFO 09-17 06:08:40 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 5104.392184 seconds
[36m(RayWorkerWrapper pid=416788, ip=10.3.0.190)[0m INFO 09-17 06:08:49 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 5113.360604 seconds
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
[36m(RayWorkerWrapper pid=2501981)[0m INFO 09-17 06:09:20 [default_loader.py:272] Loading weights took 5142.72 seconds
[36m(RayWorkerWrapper pid=2501981)[0m INFO 09-17 06:09:21 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 5145.409139 seconds
[36m(RayWorkerWrapper pid=3129981, ip=10.3.0.155)[0m INFO 09-17 06:09:41 [default_loader.py:272] Loading weights took 5163.10 seconds[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=2501982)[0m INFO 09-17 06:09:21 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 5145.498958 seconds[32m [repeated 2x across cluster][0m
[36m(RayWorkerWrapper pid=4104207, ip=10.1.0.153)[0m INFO 09-17 06:10:14 [default_loader.py:272] Loading weights took 5195.76 seconds[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=4104208, ip=10.1.0.153)[0m INFO 09-17 06:09:41 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 5165.759219 seconds[32m [repeated 6x across cluster][0m
[36m(RayWorkerWrapper pid=3117481, ip=10.1.0.95)[0m INFO 09-17 06:11:09 [default_loader.py:272] Loading weights took 5250.91 seconds[32m [repeated 5x across cluster][0m
[36m(RayWorkerWrapper pid=2324825, ip=10.1.0.114)[0m INFO 09-17 06:10:18 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 5202.552377 seconds[32m [repeated 5x across cluster][0m
[36m(RayWorkerWrapper pid=141764, ip=10.1.0.24)[0m INFO 09-17 06:12:52 [default_loader.py:272] Loading weights took 5353.73 seconds[32m [repeated 4x across cluster][0m
[36m(RayWorkerWrapper pid=3117481, ip=10.1.0.95)[0m INFO 09-17 06:11:09 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 5253.640180 seconds[32m [repeated 4x across cluster][0m
[36m(RayWorkerWrapper pid=1810894, ip=10.3.0.172)[0m lrdn0892:1810894:1810894 [3] NCCL INFO Comm config Blocking set to 1
[36m(RayWorkerWrapper pid=1810894, ip=10.3.0.172)[0m lrdn0892:1810894:1816286 [3] NCCL INFO Using network IB
[36m(RayWorkerWrapper pid=1810894, ip=10.3.0.172)[0m lrdn0892:1810894:1816286 [3] NCCL INFO ncclCommInitRankConfig comm 0x30cb2fc0 rank 55 nranks 64 cudaDev 3 nvmlDev 3 busId c8000 commId 0x6f120844a00a1752 - Init START
[36m(RayWorkerWrapper pid=1810894, ip=10.3.0.172)[0m lrdn0892:1810894:1816286 [3] NCCL INFO Bootstrap timings total 0.006841 (create 0.000017, send 0.000101, recv 0.001870, ring 0.004742, delay 0.000000)
[36m(RayWorkerWrapper pid=141766, ip=10.1.0.24)[0m INFO 09-17 06:12:52 [default_loader.py:272] Loading weights took 5354.19 seconds[32m [repeated 3x across cluster][0m
[36m(RayWorkerWrapper pid=141764, ip=10.1.0.24)[0m INFO 09-17 06:12:52 [gpu_model_runner.py:1624] Model loading took 12.1985 GiB and 5356.427808 seconds[32m [repeated 4x across cluster][0m
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m lrdn0040:2441589:2446628 [0] NCCL INFO Setting affinity for GPU 0 to ffff
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m lrdn0040:2441589:2446628 [0] NCCL INFO comm 0x30e7a8b0 rank 24 nRanks 64 nNodes 16 localRanks 4 localRank 0 MNNVL 0
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m lrdn0040:2441589:2446628 [0] NCCL INFO Trees [0] 25/-1/-1->24->27 [1] 25/-1/-1->24->27 [2] 25/-1/-1->24->27 [3] 25/-1/-1->24->27
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m lrdn0040:2441589:2446628 [0] NCCL INFO P2P Chunksize set to 131072
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m lrdn0040:2441589:2446635 [0] NCCL INFO [Proxy Service] Device 0 CPU core 8
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m lrdn0040:2441589:2446636 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 9
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m lrdn0040:2441589:2446628 [0] NCCL INFO Channel 01/0 : 24[0] -> 26[2] via P2P/IPC/read
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m lrdn0040:2441589:2446628 [0] NCCL INFO Channel 03/0 : 24[0] -> 26[2] via P2P/IPC/read
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m lrdn0040:2441589:2446628 [0] NCCL INFO Channel 00/0 : 24[0] -> 27[3] via P2P/IPC/read
[36m(RayWorkerWrapper pid=2441589, ip=10.1.0.40)[0m lrdn0040:2441589:2446628 [0] NCCL INFO Channel 02/0 : 24[0] -> 27[3] via P2P/IPC/read
[36m(RayWorkerWrapper pid=141766, ip=10.1.0.24)[0m lrdn0024:141766:149757 [3] NCCL INFO [Proxy Progress] Device 3 CPU core 11
[36m(RayWorkerWrapper pid=141766, ip=10.1.0.24)[0m lrdn0024:141766:149746 [3] NCCL INFO Channel 00/0 : 19[3] -> 22[2] [send] via NET/IB/2
[36m(RayWorkerWrapper pid=141766, ip=10.1.0.24)[0m lrdn0024:141766:149746 [3] NCCL INFO Channel 02/0 : 19[3] -> 22[2] [send] via NET/IB/2
[36m(RayWorkerWrapper pid=141766, ip=10.1.0.24)[0m lrdn0024:141766:149746 [3] NCCL INFO Channel 01/0 : 14[2] -> 19[3] [receive] via NET/IB/3
[36m(RayWorkerWrapper pid=141766, ip=10.1.0.24)[0m lrdn0024:141766:149746 [3] NCCL INFO Channel 03/0 : 14[2] -> 19[3] [receive] via NET/IB/3
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2518696 [0] NCCL INFO Channel 00/04 :  0  3  6  5  4  7 10  9  8 11 14 13 12 15 18 17 16 19 22 21 20 23 26 25 24 27 30 29 28 31 34 33 32 35 38 37 ...
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2518696 [0] NCCL INFO Channel 01/04 :  0  2  7  5  4  6 11  9  8 10 15 13 12 14 19 17 16 18 23 21 20 22 27 25 24 26 31 29 28 30 35 33 32 34 39 37 ...
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2518696 [0] NCCL INFO Channel 02/04 :  0  3  6  5  4  7 10  9  8 11 14 13 12 15 18 17 16 19 22 21 20 23 26 25 24 27 30 29 28 31 34 33 32 35 38 37 ...
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2518696 [0] NCCL INFO Channel 03/04 :  0  2  7  5  4  6 11  9  8 10 15 13 12 14 19 17 16 18 23 21 20 22 27 25 24 26 31 29 28 30 35 33 32 34 39 37 ...
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2518696 [0] NCCL INFO Check P2P Type intraNodeP2pSupport 1 directMode 0
[36m(RayWorkerWrapper pid=3807873, ip=10.1.0.79)[0m lrdn0079:3807873:3812872 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 0
[36m(RayWorkerWrapper pid=744346, ip=10.1.0.33)[0m lrdn0033:744346:749347 [1] NCCL INFO Connected all trees
[36m(RayWorkerWrapper pid=744346, ip=10.1.0.33)[0m lrdn0033:744346:749347 [1] NCCL INFO threadThresholds 8/8/64 | 512/8/64 | 512 | 512
[36m(RayWorkerWrapper pid=744346, ip=10.1.0.33)[0m lrdn0033:744346:749347 [1] NCCL INFO 4 coll channels, 4 collnet channels, 0 nvls channels, 4 p2p channels, 1 p2p channels per peer
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2518696 [0] NCCL INFO CC Off, workFifoBytes 1048576
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2518696 [0] NCCL INFO ncclCommInitRankConfig comm 0x301ab9d0 rank 0 nranks 64 cudaDev 0 nvmlDev 0 busId 1d000 commId 0x6f120844a00a1752 - Init COMPLETE
[36m(RayWorkerWrapper pid=2501972)[0m lrdn0006:2501972:2518696 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 64 total 0.28 (kernels 0.00, alloc 0.00, bootstrap 0.01, allgathers 0.02, topo 0.05, graphs 0.01, connections 0.17, rest 0.01)
[36m(RayWorkerWrapper pid=416788, ip=10.3.0.190)[0m INFO 09-17 06:13:47 [gpu_worker.py:227] Available KV cache memory: 43.22 GiB
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,480 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.52x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 720,448 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.51x
INFO 09-17 06:13:48 [kv_cache_utils.py:715] GPU KV cache size: 719,312 tokens
INFO 09-17 06:13:48 [kv_cache_utils.py:719] Maximum concurrency for 32,000 tokens per request: 22.48x
INFO 09-17 06:13:51 [core.py:171] init engine (profile, create kv cache, warmup model) took 57.76 seconds
INFO 09-17 06:13:52 [loggers.py:137] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 44957
WARNING 09-17 06:13:53 [config.py:1363] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
INFO 09-17 06:13:53 [serving_chat.py:118] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 09-17 06:13:53 [serving_completion.py:66] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}
INFO 09-17 06:13:53 [api_server.py:1349] Starting vLLM API server 0 on http://0.0.0.0:2950
INFO 09-17 06:13:53 [launcher.py:29] Available routes are:
INFO 09-17 06:13:53 [launcher.py:37] Route: /openapi.json, Methods: HEAD, GET
INFO 09-17 06:13:53 [launcher.py:37] Route: /docs, Methods: HEAD, GET
INFO 09-17 06:13:53 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: HEAD, GET
INFO 09-17 06:13:53 [launcher.py:37] Route: /redoc, Methods: HEAD, GET
INFO 09-17 06:13:53 [launcher.py:37] Route: /health, Methods: GET
INFO 09-17 06:13:53 [launcher.py:37] Route: /load, Methods: GET
INFO 09-17 06:13:53 [launcher.py:37] Route: /ping, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /ping, Methods: GET
INFO 09-17 06:13:53 [launcher.py:37] Route: /tokenize, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /detokenize, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /v1/models, Methods: GET
INFO 09-17 06:13:53 [launcher.py:37] Route: /version, Methods: GET
INFO 09-17 06:13:53 [launcher.py:37] Route: /v1/chat/completions, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /v1/completions, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /v1/embeddings, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /pooling, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /classify, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /score, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /v1/score, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /rerank, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /v1/rerank, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /v2/rerank, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /invocations, Methods: POST
INFO 09-17 06:13:53 [launcher.py:37] Route: /metrics, Methods: GET
INFO:     Started server process [2500764]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     127.0.0.1:33812 - "GET /v1/models HTTP/1.1" 200 OK
Starting sending requests to the Llama-405b model
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0INFO 09-17 06:13:57 [chat_utils.py:420] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.
ERROR 09-17 06:13:57 [serving_chat.py:200] Error in preprocessing prompt inputs
ERROR 09-17 06:13:57 [serving_chat.py:200] Traceback (most recent call last):
ERROR 09-17 06:13:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py", line 183, in create_chat_completion
ERROR 09-17 06:13:57 [serving_chat.py:200]     ) = await self._preprocess_chat(
ERROR 09-17 06:13:57 [serving_chat.py:200]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:13:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py", line 811, in _preprocess_chat
ERROR 09-17 06:13:57 [serving_chat.py:200]     request_prompt = apply_hf_chat_template(
ERROR 09-17 06:13:57 [serving_chat.py:200]                      ^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:13:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/utils.py", line 1267, in inner
ERROR 09-17 06:13:57 [serving_chat.py:200]     return fn(*args, **kwargs)
ERROR 09-17 06:13:57 [serving_chat.py:200]            ^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:13:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/chat_utils.py", line 1233, in apply_hf_chat_template
ERROR 09-17 06:13:57 [serving_chat.py:200]     raise ValueError(
ERROR 09-17 06:13:57 [serving_chat.py:200] ValueError: As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py:201: RuntimeWarning: coroutine 'AsyncMultiModalItemTracker.all_mm_data' was never awaited
  return self.create_error_response(f"{e} {e.__cause__}")
RuntimeWarning: Enable tracemalloc to get the object allocation traceback
INFO:     127.0.0.1:33820 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request

100   523  100   228  100   295    519    671 --:--:-- --:--:-- --:--:--  1191
{"object":"error","message":"As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one. None","type":"BadRequestError","param":null,"code":400}
real	0m0.443s
user	0m0.003s
sys	0m0.002s
Parallel requests
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed

  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0ERROR 09-17 06:14:57 [serving_chat.py:200] Error in preprocessing prompt inputs
ERROR 09-17 06:14:57 [serving_chat.py:200] Traceback (most recent call last):
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py", line 183, in create_chat_completion
ERROR 09-17 06:14:57 [serving_chat.py:200]     ) = await self._preprocess_chat(
ERROR 09-17 06:14:57 [serving_chat.py:200]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py", line 811, in _preprocess_chat
ERROR 09-17 06:14:57 [serving_chat.py:200]     request_prompt = apply_hf_chat_template(
ERROR 09-17 06:14:57 [serving_chat.py:200]                      ^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/utils.py", line 1267, in inner
ERROR 09-17 06:14:57 [serving_chat.py:200]     return fn(*args, **kwargs)
ERROR 09-17 06:14:57 [serving_chat.py:200]            ^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/chat_utils.py", line 1233, in apply_hf_chat_template
ERROR 09-17 06:14:57 [serving_chat.py:200]     raise ValueError(
ERROR 09-17 06:14:57 [serving_chat.py:200] ValueError: As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.
INFO:     127.0.0.1:54010 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request
ERROR 09-17 06:14:57 [serving_chat.py:200] Error in preprocessing prompt inputs
ERROR 09-17 06:14:57 [serving_chat.py:200] Traceback (most recent call last):
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py", line 183, in create_chat_completion
ERROR 09-17 06:14:57 [serving_chat.py:200]     ) = await self._preprocess_chat(
ERROR 09-17 06:14:57 [serving_chat.py:200]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py", line 811, in _preprocess_chat
ERROR 09-17 06:14:57 [serving_chat.py:200]     request_prompt = apply_hf_chat_template(
ERROR 09-17 06:14:57 [serving_chat.py:200]                      ^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/utils.py", line 1267, in inner
ERROR 09-17 06:14:57 [serving_chat.py:200]     return fn(*args, **kwargs)
ERROR 09-17 06:14:57 [serving_chat.py:200]            ^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/chat_utils.py", line 1233, in apply_hf_chat_template
ERROR 09-17 06:14:57 [serving_chat.py:200]     raise ValueError(
ERROR 09-17 06:14:57 [serving_chat.py:200] ValueError: As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.

100   523  100   228  100   295  76000  98333 --:--:-- --:--:-- --:--:--  170k
{"object":"error","message":"As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one. None","type":"BadRequestError","param":null,"code":400}ERROR 09-17 06:14:57 [serving_chat.py:200] Error in preprocessing prompt inputs
ERROR 09-17 06:14:57 [serving_chat.py:200] Traceback (most recent call last):
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_chat.py", line 183, in create_chat_completion
ERROR 09-17 06:14:57 [serving_chat.py:200]     ) = await self._preprocess_chat(
ERROR 09-17 06:14:57 [serving_chat.py:200]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_engine.py", line 811, in _preprocess_chat
ERROR 09-17 06:14:57 [serving_chat.py:200]     request_prompt = apply_hf_chat_template(
ERROR 09-17 06:14:57 [serving_chat.py:200]                      ^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/utils.py", line 1267, in inner
ERROR 09-17 06:14:57 [serving_chat.py:200]     return fn(*args, **kwargs)
ERROR 09-17 06:14:57 [serving_chat.py:200]            ^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:14:57 [serving_chat.py:200]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/entrypoints/chat_utils.py", line 1233, in apply_hf_chat_template
ERROR 09-17 06:14:57 [serving_chat.py:200]     raise ValueError(
ERROR 09-17 06:14:57 [serving_chat.py:200] ValueError: As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one.
INFO:     127.0.0.1:54022 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request

100   523  100   228  100   295  57000  73750 --:--:-- --:--:-- --:--:--  170k
{"object":"error","message":"As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one. None","type":"BadRequestError","param":null,"code":400}INFO:     127.0.0.1:54026 - "POST /v1/chat/completions HTTP/1.1" 400 Bad Request

Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
100   523  100   228  100   295  57000  73750 --:--:-- --:--:-- --:--:--  127k
{"object":"error","message":"As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the tokenizer does not define one. None","type":"BadRequestError","param":null,"code":400}Requests sent
Running concurrency level 50
INFO 09-17 06:18:56 [logger.py:43] Received request cmpl-8945c8c42f7a4468886842dcb8422b26-0: prompt: 'Do you know the book Traction by Gino Wickman', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=120, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: [128000, 5519, 499, 1440, 279, 2363, 350, 16597, 555, 480, 3394, 75206, 1543], prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:49366 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 09-17 06:18:56 [async_llm.py:271] Added request cmpl-8945c8c42f7a4468886842dcb8422b26-0.
INFO 09-17 06:18:56 [ray_distributed_executor.py:562] VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE = auto
INFO 09-17 06:18:56 [ray_distributed_executor.py:564] VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM = False
INFO 09-17 06:18:56 [ray_distributed_executor.py:579] RAY_CGRAPH_get_timeout is set to 300
ERROR 09-17 06:19:17 [dump_input.py:69] Dumping input data
ERROR 09-17 06:19:17 [dump_input.py:71] V1 LLM engine (v0.9.1) with config: model='/leonardo_scratch/large/userinternal/lcavall1/llama405_mlperf/model//Llama-3.1-405B', speculative_config=None, tokenizer='/leonardo_scratch/large/userinternal/lcavall1/llama405_mlperf/model//Llama-3.1-405B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=64, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/leonardo_scratch/large/userinternal/lcavall1/llama405_mlperf/model//Llama-3.1-405B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=False, pooler_config=None, compilation_config={"level":0,"debug_dump_path":"","cache_dir":"","backend":"","custom_ops":[],"splitting_ops":[],"use_inductor":true,"compile_sizes":[],"inductor_compile_config":{"enable_auto_functionalized_v2":false},"inductor_passes":{},"use_cudagraph":true,"cudagraph_num_of_warmups":0,"cudagraph_capture_sizes":[],"cudagraph_copy_inputs":false,"full_cuda_graph":false,"max_capture_size":0,"local_cache_dir":null}, 
ERROR 09-17 06:19:17 [dump_input.py:79] Dumping scheduler output for model execution:
ERROR 09-17 06:19:17 [dump_input.py:80] SchedulerOutput(scheduled_new_reqs=[NewRequestData(req_id=cmpl-8945c8c42f7a4468886842dcb8422b26-0,prompt_token_ids_len=13,mm_inputs=[],mm_hashes=[],mm_positions=[],sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=120, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None),block_ids=([1],),num_computed_tokens=0,lora_request=None)], scheduled_cached_reqs=[], num_scheduled_tokens={cmpl-8945c8c42f7a4468886842dcb8422b26-0: 13}, total_num_scheduled_tokens=13, scheduled_spec_decode_tokens={}, scheduled_encoder_inputs={}, num_common_prefix_blocks=[1], finished_req_ids=[], free_encoder_input_ids=[], structured_output_request_ids={}, grammar_bitmask=null, kv_connector_metadata=null)
ERROR 09-17 06:19:17 [dump_input.py:82] SchedulerStats(num_running_reqs=1, num_waiting_reqs=0, gpu_cache_usage=4.448695420067317e-05, prefix_cache_stats=PrefixCacheStats(reset=False, requests=1, queries=13, hits=0), spec_decoding_stats=None)
ERROR 09-17 06:19:17 [core.py:517] EngineCore encountered a fatal error.
ERROR 09-17 06:19:17 [core.py:517] Traceback (most recent call last):
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/ray/dag/compiled_dag_node.py", line 2344, in _execute_until
ERROR 09-17 06:19:17 [core.py:517]     result = self._dag_output_fetcher.read(timeout)
ERROR 09-17 06:19:17 [core.py:517]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/ray/experimental/channel/common.py", line 318, in read
ERROR 09-17 06:19:17 [core.py:517]     outputs = self._read_list(timeout)
ERROR 09-17 06:19:17 [core.py:517]               ^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/ray/experimental/channel/common.py", line 409, in _read_list
ERROR 09-17 06:19:17 [core.py:517]     raise e
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/ray/experimental/channel/common.py", line 391, in _read_list
ERROR 09-17 06:19:17 [core.py:517]     result = c.read(min(remaining_timeout, iteration_timeout))
ERROR 09-17 06:19:17 [core.py:517]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/ray/experimental/channel/shared_memory_channel.py", line 776, in read
ERROR 09-17 06:19:17 [core.py:517]     return self._channel_dict[self._resolve_actor_id()].read(timeout)
ERROR 09-17 06:19:17 [core.py:517]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/ray/experimental/channel/shared_memory_channel.py", line 612, in read
ERROR 09-17 06:19:17 [core.py:517]     output = self._buffers[self._next_read_index].read(timeout)
ERROR 09-17 06:19:17 [core.py:517]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/ray/experimental/channel/shared_memory_channel.py", line 480, in read
ERROR 09-17 06:19:17 [core.py:517]     ret = self._worker.get_objects(
ERROR 09-17 06:19:17 [core.py:517]           ^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/ray/_private/worker.py", line 893, in get_objects
ERROR 09-17 06:19:17 [core.py:517]     ] = self.core_worker.get_objects(
ERROR 09-17 06:19:17 [core.py:517]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [core.py:517]   File "python/ray/_raylet.pyx", line 3189, in ray._raylet.CoreWorker.get_objects
ERROR 09-17 06:19:17 [core.py:517]   File "python/ray/includes/common.pxi", line 106, in ray._raylet.check_status
ERROR 09-17 06:19:17 [core.py:517] ray.exceptions.RayChannelTimeoutError: System error: Timed out waiting for object available to read. ObjectID: 00690ed4cc8b1c84d40a7272d3066bf8f7dcf2910200000002e1f505
ERROR 09-17 06:19:17 [core.py:517] 
ERROR 09-17 06:19:17 [core.py:517] The above exception was the direct cause of the following exception:
ERROR 09-17 06:19:17 [core.py:517] 
ERROR 09-17 06:19:17 [core.py:517] Traceback (most recent call last):
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 508, in run_engine_core
ERROR 09-17 06:19:17 [core.py:517]     engine_core.run_busy_loop()
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 535, in run_busy_loop
ERROR 09-17 06:19:17 [core.py:517]     self._process_engine_step()
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 560, in _process_engine_step
ERROR 09-17 06:19:17 [core.py:517]     outputs, model_executed = self.step_fn()
ERROR 09-17 06:19:17 [core.py:517]                               ^^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 231, in step
ERROR 09-17 06:19:17 [core.py:517]     model_output = self.execute_model(scheduler_output)
ERROR 09-17 06:19:17 [core.py:517]                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 217, in execute_model
ERROR 09-17 06:19:17 [core.py:517]     raise err
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 211, in execute_model
ERROR 09-17 06:19:17 [core.py:517]     return self.model_executor.execute_model(scheduler_output)
ERROR 09-17 06:19:17 [core.py:517]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/v1/executor/ray_distributed_executor.py", line 58, in execute_model
ERROR 09-17 06:19:17 [core.py:517]     return refs[0].get()
ERROR 09-17 06:19:17 [core.py:517]            ^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/ray/experimental/compiled_dag_ref.py", line 124, in get
ERROR 09-17 06:19:17 [core.py:517]     self._dag._execute_until(
ERROR 09-17 06:19:17 [core.py:517]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/ray/dag/compiled_dag_node.py", line 2350, in _execute_until
ERROR 09-17 06:19:17 [core.py:517]     raise RayChannelTimeoutError(
ERROR 09-17 06:19:17 [core.py:517] ray.exceptions.RayChannelTimeoutError: System error: If the execution is expected to take a long time, increase RAY_CGRAPH_get_timeout which is currently 10 seconds. Otherwise, this may indicate that the execution is hanging.
INFO 09-17 06:19:17 [ray_distributed_executor.py:128] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.
2025-09-17 06:19:17,358	INFO compiled_dag_node.py:2109 -- Tearing down compiled DAG
2025-09-17 06:19:17,359	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 250e70aaeb144e1a8b26ab6a02000000)
2025-09-17 06:19:17,359	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e332ce045572f114dfe4843402000000)
2025-09-17 06:19:17,359	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, f820b8f841699c6fe751360302000000)
2025-09-17 06:19:17,359	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e091aad5c3a40daa75fdeabf02000000)
2025-09-17 06:19:17,359	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, c1379e73e42064197950efc402000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, f9833dcb790e2f9f7f02bb1e02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, ae3eb63adc85631d81b8176102000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 97ae059db2e608b7a38c0b7e02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, fcccdcaa2836ce9be72eb41f02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 1d85a46342d22b3a0953ba2402000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 6ad8b19d75306c92349f74dd02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, ef47a93561934a648194ff4402000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 0e4b027138964a25a94da00e02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 334de405932f8b981b84eb2802000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, f73ca9357950d7a377479a0602000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, f01e6ac05f91c5f374085b5002000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 1ccecfbf43b5518720e8a09602000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 79604d48b7651581d7a18a7302000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 17dcb12bc616151fd452f2dd02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, c68dc9c558dcc9560fe2cf4702000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 36b12575ee996368a87e229602000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 75fed59c5b6761a182e0843b02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, c3ca2ce1298a1deb5003fcdf02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 2ab793b6fd21a16023e89fb602000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 598f69cbfea7dd1d098774ee02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 5832072e8b840331287350a402000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e42b21f396c5623558e7082402000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, bbc5650e38c368da2ccdc76302000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 3bd5c5e79be15e51ab906a5502000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 4377b190d72b5bbadd6ed1d702000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 34b19ceeadac67d0690caed202000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 8658ba02141965fed287472c02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 38a319a2e52406794f9e8f3f02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 19f29962231d740d659b48c302000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e783c42aa25565d5dfe5760c02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 2dd4616609ef686e5c8b802a02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, d326607671732a42418bd77f02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 2c5cd718582965c00399d56a02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 39c3518013d1476d57973b4402000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 87d39c9842ac7e6c2419209002000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, d9f4055aae23bfdeb739c83c02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 07bca87b9d9b9a668f1cf73902000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, d40a7272d3066bf8f7dcf29102000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 9a65dd2463baa9da1bc6381f02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 89b8e5d39fe3c39492289a3402000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, b4aaf1a864af134d8214d3f202000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 86a125baf8fa221ddf556a3a02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 590039ddc39add73befd532a02000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 8a2728b7074d4ee2f1b9ab9002000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 2536ed5010f499537bc2c25302000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 6c3067a95b4e126e95df269002000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 68b95a99c415bfe14fb525f802000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 9c60cdef14eaa521bf76a64302000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, a4942e478b7453c09fddcbe902000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 8025830bd43ce2671eb279d902000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 12dfce2a11a6646d8b09f7e702000000)
2025-09-17 06:19:17,360	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 5a9e9e31d52b9fdb739b563d02000000)
2025-09-17 06:19:17,361	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 9b0b97cc83ceb0de9fa57b6d02000000)
2025-09-17 06:19:17,361	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 268d52a2f7a29795a5c5286b02000000)
2025-09-17 06:19:17,361	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 7c6e5c87c7a88f0814c0e35302000000)
2025-09-17 06:19:17,361	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 3bd09e6ff8da98e56c06ba9302000000)
2025-09-17 06:19:17,361	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 3cedf0b21524fe75c9a5e90d02000000)
2025-09-17 06:19:17,361	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 95bf8c16ae6ad26eb385fc0602000000)
2025-09-17 06:19:17,361	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 01a031504ad80d2cec98915f02000000)
ERROR 09-17 06:19:17 [async_llm.py:420] AsyncLLM output_handler failed.
ERROR 09-17 06:19:17 [async_llm.py:420] Traceback (most recent call last):
ERROR 09-17 06:19:17 [async_llm.py:420]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/v1/engine/async_llm.py", line 379, in output_handler
ERROR 09-17 06:19:17 [async_llm.py:420]     outputs = await engine_core.get_output_async()
ERROR 09-17 06:19:17 [async_llm.py:420]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ERROR 09-17 06:19:17 [async_llm.py:420]   File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/vllm/v1/engine/core_client.py", line 790, in get_output_async
ERROR 09-17 06:19:17 [async_llm.py:420]     raise self._format_exception(outputs) from None
ERROR 09-17 06:19:17 [async_llm.py:420] vllm.v1.engine.exceptions.EngineDeadError: EngineCore encountered an issue. See stack trace (above) for the root cause.
INFO 09-17 06:19:17 [async_llm.py:346] Request cmpl-8945c8c42f7a4468886842dcb8422b26-0 failed (engine dead).
Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 1231, in <module>
    main(args)
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 879, in main
    benchmark_result = asyncio.run(
                       ^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 568, in benchmark
    raise ValueError(
ValueError: Initial test run failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/backend_request_func.py", line 503, in async_request_openai_completions
    if data["choices"][0]["text"]:
       ~~~~^^^^^^^^^^^
KeyError: 'choices'

2025-09-17 06:19:17,710	INFO compiled_dag_node.py:2137 -- Waiting for worker tasks to exit
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [2500764]
*** SIGTERM received at time=1758082759 on cpu 24 ***
PC: @     0x1483db4ad7aa  (unknown)  pthread_cond_timedwait@@GLIBC_2.3.2
    @     0x1483db4b1cf0  (unknown)  (unknown)
[2025-09-17 06:19:19,301 E 2501813 2501813] logging.cc:484: *** SIGTERM received at time=1758082759 on cpu 24 ***
[2025-09-17 06:19:19,301 E 2501813 2501813] logging.cc:484: PC: @     0x1483db4ad7aa  (unknown)  pthread_cond_timedwait@@GLIBC_2.3.2
[2025-09-17 06:19:19,301 E 2501813 2501813] logging.cc:484:     @     0x1483db4b1cf0  (unknown)  (unknown)
2025-09-17 06:19:19,711	INFO compiled_dag_node.py:2109 -- Tearing down compiled DAG
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 250e70aaeb144e1a8b26ab6a02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e332ce045572f114dfe4843402000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, f820b8f841699c6fe751360302000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e091aad5c3a40daa75fdeabf02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, c1379e73e42064197950efc402000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, f9833dcb790e2f9f7f02bb1e02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, ae3eb63adc85631d81b8176102000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 97ae059db2e608b7a38c0b7e02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, fcccdcaa2836ce9be72eb41f02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 1d85a46342d22b3a0953ba2402000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 6ad8b19d75306c92349f74dd02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, ef47a93561934a648194ff4402000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 0e4b027138964a25a94da00e02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 334de405932f8b981b84eb2802000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, f73ca9357950d7a377479a0602000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, f01e6ac05f91c5f374085b5002000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 1ccecfbf43b5518720e8a09602000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 79604d48b7651581d7a18a7302000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 17dcb12bc616151fd452f2dd02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, c68dc9c558dcc9560fe2cf4702000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 36b12575ee996368a87e229602000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 75fed59c5b6761a182e0843b02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, c3ca2ce1298a1deb5003fcdf02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 2ab793b6fd21a16023e89fb602000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 598f69cbfea7dd1d098774ee02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 5832072e8b840331287350a402000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e42b21f396c5623558e7082402000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, bbc5650e38c368da2ccdc76302000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 3bd5c5e79be15e51ab906a5502000000)
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 4377b190d72b5bbadd6ed1d702000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 34b19ceeadac67d0690caed202000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 8658ba02141965fed287472c02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 38a319a2e52406794f9e8f3f02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 19f29962231d740d659b48c302000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, e783c42aa25565d5dfe5760c02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 2dd4616609ef686e5c8b802a02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, d326607671732a42418bd77f02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 2c5cd718582965c00399d56a02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 39c3518013d1476d57973b4402000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 87d39c9842ac7e6c2419209002000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, d9f4055aae23bfdeb739c83c02000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 07bca87b9d9b9a668f1cf73902000000)
2025-09-17 06:19:19,712	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, d40a7272d3066bf8f7dcf29102000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 9a65dd2463baa9da1bc6381f02000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 89b8e5d39fe3c39492289a3402000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, b4aaf1a864af134d8214d3f202000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 86a125baf8fa221ddf556a3a02000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 590039ddc39add73befd532a02000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 8a2728b7074d4ee2f1b9ab9002000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 2536ed5010f499537bc2c25302000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 6c3067a95b4e126e95df269002000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 68b95a99c415bfe14fb525f802000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 9c60cdef14eaa521bf76a64302000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, a4942e478b7453c09fddcbe902000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 8025830bd43ce2671eb279d902000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 12dfce2a11a6646d8b09f7e702000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 5a9e9e31d52b9fdb739b563d02000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 9b0b97cc83ceb0de9fa57b6d02000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 268d52a2f7a29795a5c5286b02000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 7c6e5c87c7a88f0814c0e35302000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 3bd09e6ff8da98e56c06ba9302000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 3cedf0b21524fe75c9a5e90d02000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 95bf8c16ae6ad26eb385fc0602000000)
2025-09-17 06:19:19,713	INFO compiled_dag_node.py:2115 -- Cancelling compiled worker on actor: Actor(RayWorkerWrapper, 01a031504ad80d2cec98915f02000000)
2025-09-17 06:19:19,986	INFO compiled_dag_node.py:2137 -- Waiting for worker tasks to exit
Running concurrency level 100
Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 1231, in <module>
    main(args)
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 879, in main
    benchmark_result = asyncio.run(
                       ^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 568, in benchmark
    raise ValueError(
ValueError: Initial test run failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1091, in _wrap_create_connection
    sock = await aiohappyeyeballs.start_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 123, in start_connection
    raise OSError(first_errno, msg)
ConnectionRefusedError: [Errno 111] Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/backend_request_func.py", line 484, in async_request_openai_completions
    async with session.post(url=api_url, json=payload,
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 1359, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 663, in _request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 563, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1032, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1366, in _create_direct_connection
    raise last_exc
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1335, in _create_direct_connection
    transp, proto = await self._wrap_create_connection(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1106, in _wrap_create_connection
    raise client_error(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host localhost:2950 ssl:default [Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)]

Running concurrency level 200
Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 1231, in <module>
    main(args)
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 879, in main
    benchmark_result = asyncio.run(
                       ^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 568, in benchmark
    raise ValueError(
ValueError: Initial test run failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1091, in _wrap_create_connection
    sock = await aiohappyeyeballs.start_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 123, in start_connection
    raise OSError(first_errno, msg)
ConnectionRefusedError: [Errno 111] Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/backend_request_func.py", line 484, in async_request_openai_completions
    async with session.post(url=api_url, json=payload,
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 1359, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 663, in _request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 563, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1032, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1366, in _create_direct_connection
    raise last_exc
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1335, in _create_direct_connection
    transp, proto = await self._wrap_create_connection(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1106, in _wrap_create_connection
    raise client_error(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host localhost:2950 ssl:default [Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)]

Running concurrency level 300
Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 1231, in <module>
    main(args)
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 879, in main
    benchmark_result = asyncio.run(
                       ^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 118, in run
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 568, in benchmark
    raise ValueError(
ValueError: Initial test run failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1091, in _wrap_create_connection
    sock = await aiohappyeyeballs.start_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 123, in start_connection
    raise OSError(first_errno, msg)
ConnectionRefusedError: [Errno 111] Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/backend_request_func.py", line 484, in async_request_openai_completions
    async with session.post(url=api_url, json=payload,
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 1359, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 663, in _request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 563, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1032, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1366, in _create_direct_connection
    raise last_exc
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1335, in _create_direct_connection
    transp, proto = await self._wrap_create_connection(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1106, in _wrap_create_connection
    raise client_error(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host localhost:2950 ssl:default [Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)]

Running concurrency level 400
Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 1231, in <module>
    main(args)
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 879, in main
    benchmark_result = asyncio.run(
                       ^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 568, in benchmark
    raise ValueError(
ValueError: Initial test run failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1091, in _wrap_create_connection
    sock = await aiohappyeyeballs.start_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 123, in start_connection
    raise OSError(first_errno, msg)
ConnectionRefusedError: [Errno 111] Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/backend_request_func.py", line 484, in async_request_openai_completions
    async with session.post(url=api_url, json=payload,
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 1359, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 663, in _request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 563, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1032, in _create_connection
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
Don't kill process 2497730, it is still running
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1366, in _create_direct_connection
    raise last_exc
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1335, in _create_direct_connection
    transp, proto = await self._wrap_create_connection(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1106, in _wrap_create_connection
    raise client_error(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host localhost:2950 ssl:default [Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)]

Running concurrency level 500
Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 1231, in <module>
    main(args)
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 879, in main
    benchmark_result = asyncio.run(
                       ^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 568, in benchmark
    raise ValueError(
ValueError: Initial test run failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1091, in _wrap_create_connection
    sock = await aiohappyeyeballs.start_connection(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 123, in start_connection
    raise OSError(first_errno, msg)
ConnectionRefusedError: [Errno 111] Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/backend_request_func.py", line 484, in async_request_openai_completions
    async with session.post(url=api_url, json=payload,
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 1359, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 663, in _request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 563, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1032, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1366, in _create_direct_connection
    raise last_exc
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1335, in _create_direct_connection
    transp, proto = await self._wrap_create_connection(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1106, in _wrap_create_connection
    raise client_error(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host localhost:2950 ssl:default [Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)]

Running concurrency level 1000
Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 1231, in <module>
    main(args)
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 879, in main
    benchmark_result = asyncio.run(
                       ^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 190, in run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo/prod/spack/06/install/0.22/linux-rhel8-icelake/gcc-8.5.0/python-3.11.7-ziwh63aulhhzxksf42k5u3gnim2rbpmp/lib/python3.11/asyncio/base_events.py", line 653, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/benchmark_serving.py", line 568, in benchmark
    raise ValueError(
ValueError: Initial test run failed - Please make sure benchmark arguments are correctly specified. Error: Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1091, in _wrap_create_connection
    sock = await aiohappyeyeballs.start_connection(
Don't kill process 2497730, it is still running
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohappyeyeballs/impl.py", line 123, in start_connection
    raise OSError(first_errno, msg)
ConnectionRefusedError: [Errno 111] Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/benchmarks/backend_request_func.py", line 484, in async_request_openai_completions
    async with session.post(url=api_url, json=payload,
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 1359, in __aenter__
    self._resp: _RetType = await self._coro
                           ^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/client.py", line 663, in _request
    conn = await self._connector.connect(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 563, in connect
    proto = await self._create_connection(req, traces, timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1032, in _create_connection
    _, proto = await self._create_direct_connection(req, traces, timeout)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1366, in _create_direct_connection
    raise last_exc
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1335, in _create_direct_connection
    transp, proto = await self._wrap_create_connection(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/leonardo_scratch/large/userinternal/rscheda0/benchmarks/inference/inference-Leonardo/env_vllm/lib/python3.11/site-packages/aiohttp/connector.py", line 1106, in _wrap_create_connection
    raise client_error(req.connection_key, exc) from exc
aiohttp.client_exceptions.ClientConnectorError: Cannot connect to host localhost:2950 ssl:default [Multiple exceptions: [Errno 111] Connect call failed ('::1', 2950, 0, 0), [Errno 111] Connect call failed ('127.0.0.1', 2950)]

2025-09-17 06:21:21,167	SUCC cli.py:63 -- [32m------------------------------------------[39m
2025-09-17 06:21:21,167	SUCC cli.py:64 -- [32mJob 'raysubmit_A5R76enm9BF1W89Z' succeeded[39m
2025-09-17 06:21:21,167	SUCC cli.py:65 -- [32m------------------------------------------[39m
Ray Job submitted...
Checking for running jobs...
Don't kill process 2497730, it is still running
No jobs are currently running.
Process 2497730 has finished, exiting script
